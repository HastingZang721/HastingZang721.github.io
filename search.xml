<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[详解二叉排序树（Binary Sort Tree)的删除操作]]></title>
    <url>%2F2019%2F06%2F16%2FBSTDELETE%2F</url>
    <content type="text"><![CDATA[一般构造二叉排序树的目的是为了提高查找与删除关键字的速度，而删除操作较为复杂，故今日复盘之; 一）二叉树的性质二叉搜索树又被称为二叉排序树，那么它本身也是一棵二叉树，那么满足以下性质的二叉树就是二叉搜索树： 若左子树不为空，则左子树上左右节点的值都小于根节点的值; 若它的右子树不为空，则它的右子树上所有的节点的值都大于根节点的值; 它的左右子树也要分别是二叉搜索树; 二）删除操作的三种情况 待删除节点无孩子节点 此时即叶子结点；（很容易实现删除操作，直接删除结点即可） 待删除节点只有一个孩子节点 删除结点后，将它的左子树或者右子树整个移动到删除结点的位置 待删除节点有两个孩子节点 找到待删除节点左子树的最右孩子（或者右子树的最左孩子）来替代待删除节点的位置 因为本题我随手画的 ，所以二叉排序树情况简单，两种情况复杂度相同的，但遇到个别情况还是要因题而异； 三）二叉排序树的删除操作代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162int delete_node(node *root,double a)&#123; node *p=root; if(search_value(*root,a,&amp;p)==1)&#123; //已经存在a //开始删除。p指向要删除节点的父节点 node q=null;//q指向要删除的节点 if(a&lt; p-&gt;data)&#123; q=p-&gt;lchild; &#125; else&#123; q=p-&gt;rchild; &#125; if (q-&gt;Lchild==null&amp;&amp;q-&gt;rchild==null)//若q是叶子节点则直接删除&#123;&#125; &#123; if (a &lt; p-&gt;data)//a是p的左节点 p-&gt;lchild == null; else p-&gt;rchild == null; &#125; else if((p-&gt;lchild==null&amp;&amp;p-&gt;rchild!=null)||(p-&gt;lchild!=null&amp;&amp;p-&gt;rchild==null))&#123; //只有左子树或者只有右子树 node r=(q-&gt;lchild=null?q-&gt;rchild:q-&gt;lchild);//指向不为空的子树 if(a&lt;p-&gt;lchild)//a 是p 的左子树节点 p-&gt;lchild=r; else p-&gt;rchild=r; &#125; else&#123;//q既有左子树也有右子树 //这里选择左子树上最大的节点作为替代p的节点（新的父节点） node r=q-&gt;lchild; if(r-&gt;rchild==null)&#123;//判断r de lchild 是否为空 p-&gt;lchild=r; r-&gt;lchild=q-&gt;lchild; &#125; else&#123;//右子树不为空 node r1; node r1_father; while(r1-&gt;rchild!=NULL)&#123; r1_father=r1; r1=r1-&gt;rchild; &#125;r1_father-&gt;rchild=r1-&gt;lchild; r1-&gt;lchild=r; r1=p-&gt;rchild; r1-&gt;rchild=q-&gt;rchild; &#125; &#125; free(q); q=null; return 0; &#125; return 1; &#125; 关于代码的测试可以通过中序遍历二叉排序树，检查结果是从小到大输出。 四）备忘录关于二叉排序树的查询和插入操作代码也同步在MyProjectCode欢迎大家指正和建议；同时也感谢网络开源的学习资源，路漫漫其修远兮一起加油吧；]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Data_structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从B/B+树来了解Mysql索引背后的数据结构]]></title>
    <url>%2F2019%2F06%2F16%2FBTree%2F</url>
    <content type="text"><![CDATA[在复习数据库知识中想要了解Mysql的查询原理，顺便复习算法导论第18章内容，故复盘之； 1）索引 索引是对数据库表中一列或多列的值进行排序的一种数据结构。索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录 目前Mysql目前主要有以下几种索引类型：FULLTEXT，HASH，BTREE，RTREE其中BTREE索引就是一种将索引值按一定的算法，存入一个树形的数据结构中，每次查询都是从树的入口root开始，依次遍历node，获取leaf。这是MySQL里默认和最常用的索引类型。 2）数据结构与算法2.1）B树的定义根为T.root A)每个节点x 有如下性质： x.n：表示当前存储在节点x的关键字个数； x .n个关键字本身x.key1,x.key2,…x.keyx.n，以非降序存放； x.leaf==true 表示x是叶子节点，反之 false； 每个内部节点x有n+1个指向孩子的指针 B)每个关键字x.keyi对存储在各个子树的关键字范围加以分割；C)每个叶节点具有相同的深度，即树的高度H;D)每个节点所包含的关键字个数有上界和下界。用一个被称为B树的最小度数t&gt;=2来表示这些界；(图中t=3) 注意：B树上绝大多数操作所需的磁盘存取次数与B树的高度H成正比 2.2）B+树的定义B+树作为B树的一种变形树，它与B树的差异在于： 有k个子结点的结点必然有k个关键码； 内节点不存储data，只存储key； 叶子节点不存储指针；非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录。 如图所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如果要查询key为从8到92的所有数据记录，当找到8后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率 3）原理分析 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级;所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数 3.1）主存存储原理从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响， 3.2）磁盘存储原理与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间 3.3)局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多; 机械运动耗费;因此为了提高效率，要尽量减少磁盘I/O，减少读写操作。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 文件系统及数据库系统的设计者利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建一个节点的同时，直接申请一个页的空间( 512或者1024)，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。如，将B树的度t设置为1024 4)证明依据算法导论18章知识，分析B树最坏情况下的高度： 定理： 如果n&gt;=1，那么对任意一颗包含n个关键字，高度为h,最小度数t&gt;=2的B树T有： $$h&lt;=log_t \frac{n+1} {2}$$ 证明： B树T的根至少有一个关键字，而且所有其他的节点至少包含t-1个关键字， 因此高度为h的 B树T 在深度为1的时候至少包含2个节点，在深度2时至少包含2t个节点，深度为3时至少包含$2t^2$个节点… 直到深度h至少有$2t^{h-1}$个节点 $$n&gt;=1+(t-1)\sum_{i=1}^h2t^{i-1}=2t^h-1$$两边取以t为底的对数即可证明定理 由此可以看出，B树相较于红黑树，对数的底可以取很大，在查询时节约lgt的因子；避免大量磁盘访问；]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇日志：Markdown语法学习]]></title>
    <url>%2F2019%2F06%2F16%2FMarkdown%20my%20first%20blog%2F</url>
    <content type="text"><![CDATA[开门大吉我这也是王婆卖瓜 现学现卖；首先让我们来看看Markdown都有哪些优点： 1) 纯文本，所以兼容性极强，可以用所有文本编辑器打开。2) 让你专注于文字而不是排版。3) 格式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。4) Markdown 的标记语法有极好的可读性。那么下面将简单介绍如何使用Markdown （一）标题平时我常用的的文本编辑器大多是：输入文本、选中文本、设置标题格式这样的过程。而在 Markdown 中，你只需要在文本前面加上 # 即可，同理、你还可以增加二级标题、三级标题、四级标题、五级标题和六级标题，总共六级，增加 # 起到加粗的作用，标题字号相应降低。如下： #一级标题 ##二级标题 ###三级标题 ####四级标题 #####五级标题 ######六级标题 一级标题二级标题三级标题四级标题五级标题六级标题好了， 看到这里你就算入门成功（哈哈哈开玩笑的！介绍继续） （二）斜体效果如下：我是斜体 两边各用一颗星包裹 我是粗体 两边各用两颗星包裹 我是斜体加粗 两边各用三颗星包裹 （三）列表列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 例如： 有序列表 1.文本 2.文本-3. 文本无序列表 无序列表 （四） 链接1在 Markdown 中，插入链接不需要其他按钮，你只需要使用格式：[链接名称](链接地址) 例如 zhihu （五）图片同样在 Markdown 中，插入图片不需要其他按钮，你只需要使用 ![](图片链接地址)这样的语法即可，例如：链接: IU. 图片: 若要带尺寸的图片: ![Alt](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike92%2C5%2C5%2C92%2C30/sign=ecefedb3034f78f0940692a118586130/d439b6003af33a87fe5273fdc65c10385343b51d.jpg=30x30) (六)引用写作的时引用他人的文字，只需在你希望引用的文字前面加上 &gt; 就可以 真正高明的人，就是能够借助别人的智慧，来使自己不受蒙蔽的人。 —— 苏格拉底 (七)如何插入一段漂亮的代码片代码快以”12345678```Python def my_abs(x): if x &gt;= 0: return x else: return -x &#125; （八）表格123456789101112&lt;table&gt; &lt;tr&gt; &lt;th&gt;星期一&lt;/th&gt; &lt;th&gt;星期二&lt;/th&gt; &lt;th&gt;星期三&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;董豪&lt;/td&gt; &lt;td&gt;斌豪&lt;/td&gt; &lt;td&gt;林豪&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 生成结果如下（顺序不要介意啊） 星期一 星期二 星期三 斌豪 董豪 林豪/td&gt; 或者另一种方式12345项目 | Value-------- | -----电脑 | $1600手机 | $12导管 | $1 显示结果 项目 Value 电脑 $1600 手机 $12 导管 $1 (九)生成一个适合你的列表12345678910- 项目 - 项目 - 项目1. 项目12. 项目23. 项目3- [ ] 计划任务- [x] 完成任务 项目 项目 项目 项目1 项目2 项目3 计划任务 完成任务 （十）设定内容居中、居左、居右使用:---------:居中使用:----------居左使用----------:居右| 第一列 | 第二列 | 第三列 ||:———–:| ————-:|:————-|| 第一列文本居中 | 第二列文本居右 | 第三列文本居左 | （十一）如何创建一个注脚123456789101112[^2]: 注脚的解释## (十二)KaTeX数学公式```您可以使用渲染LaTeX数学表达式 [KaTeX](https://khan.github.io/KaTeX/):Gamma公式展示 $\Gamma(n) = (n-1)!\quad\foralln\in\mathbb N$ 是通过欧拉积分\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,. Gamma公式展示 $\Gamma(n) = (n-1)!\quad\foralln\in\mathbb N$ 是通过欧拉积分 \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. 你可以找到更多关于的信息 LaTeX 数学表达式[here][1]. （十三）新的甘特图功能，丰富你的文章12345678gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section 现有任务 已完成 :done, des1, 2014-01-06,2014-01-08 进行中 :active, des2, 2014-01-09, 3d 计划一 : des3, after des2, 5d 计划二 : des4, after des3, 5d 123456789mermaidgantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section 现有任务 已完成 :done, des1, 2014-01-06,2014-01-08 进行中 :active, des2, 2014-01-09, 3d 计划一 : des3, after des2, 5d 计划二 : des4, after des3, 5d 关于 甘特图 语法，参考 [这儿][2], 总结以上是markdown简单使用案例。Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成 极富表现力 的文档，所写所得，实时渲染.用户专注于内容，无需关心对文字排版，所以深受文字工作者、运营策划人员、程序猿的喜爱。]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C+中的深复制与浅复制]]></title>
    <url>%2F2019%2F06%2F16%2FC%2B%E6%B7%B1%E6%B5%85%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[最近重刷清华大学的C++教程郑莉版看到动态分配内存这节：其实在大多数的情况下，隐含的复制构造函数足以实现对象间数据元素的一一对应复制，但并不是总适用，因为它完成的只是浅复制。所以借此例题来学习对象间的深复制与浅复制。 例题说明：使用new动态创建Point类，并创建动态数组ArrayofPoints;在main函数中利用默认的复制构造函数建立两组完全相同的点； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;iostream&gt;#include&lt;cassert&gt;using namespace std;class Point&#123;public : Point() :x(0), y(0) &#123; cout&lt;&lt;&quot;default Construstor called.&quot;&lt;&lt;endl;&#125; Point(int x,int y):x(x),y(y)&#123; cout&lt;&lt;&quot;constructor called&quot;&lt;&lt;endl; &#125; ~ Point()&#123;cout&lt;&lt;&quot;Destrustor called.&quot;&lt;&lt;endl;&#125; int getX() const &#123;return x;&#125; int getY() const &#123;return y;&#125; void move(int newX,int newY)&#123; x=newX; y=newY; &#125;private: int x, y;&#125;; class ArrayofPoints&#123; //动态数组Arrayofpoints与points类存在者使用关系 public: ArrayofPoints (int size):size(size) &#123; points=new Point[size]; &#125; ~ArrayofPoints()&#123; cout&lt;&lt;&quot;Deleting···&quot;&lt;&lt;endl; delete[] points; &#125; Point &amp;element(int index)&#123; assert(index&gt;= 0&amp;&amp; index &lt;size);// return points[index]; &#125; private: Point *points; int size; &#125;;int main()&#123; int count; cout&lt;&lt;&quot;please enter the count of points:&quot;; cin&gt;&gt;count; ArrayofPoints pointArray1(count); pointArray1.element(0).move(5,10); pointArray1.element(1).move(15,20); ArrayofPoints pointsArray2=pointArray1;//创建副本 cout&lt;&lt;&quot;Copy of pointsArray1:&quot;&lt;&lt;endl; cout&lt;&lt;&quot;point_0 of array2:&quot;&lt;&lt;pointsArray2.element(0).getX()&lt;&lt;&quot;,&quot;&lt;&lt;pointsArray2.element(0).getY()&lt;&lt;endl; cout&lt;&lt;&quot;point_1 of array2:&quot;&lt;&lt;pointsArray2.element(1).getX()&lt;&lt;&quot;,&quot;&lt;&lt;pointsArray2.element(1).getY()&lt;&lt;endl; cout&lt;&lt;&quot;----------------我是可爱的分隔符--------------&quot;&lt;&lt;endl; pointArray1.element(0).move(25,40); pointArray1.element(1).move(35,40); cout&lt;&lt; &quot;after the moving of pointsArray1&quot;&lt;&lt;endl; cout&lt;&lt;&quot;point_0 of array2:&quot;&lt;&lt;pointsArray2.element(0).getX()&lt;&lt;&quot;,&quot;&lt;&lt;pointsArray2.element(0).getY()&lt;&lt;endl; cout&lt;&lt;&quot;point_1 of array2:&quot;&lt;&lt;pointsArray2.element(1).getX()&lt;&lt;&quot;,&quot;&lt;&lt;pointsArray2.element(1).getY()&lt;&lt;endl; return 0; 运行结果如下：123456789101112131415161718please enter the count of points:2default Construstor called.default Construstor called.Copy of pointsArray1:point_0 of array2:5,10point_1 of array2:15,20----------------我是可爱的分隔符--------------after the moving of pointsArray1point_0 of array2:25,40point_1 of array2:35,40Deleting···Destrustor called.u1(6426,0x1143ee5c0) malloc: *** error for object 0x7fce4ac02a40: pointer being freed was not allocatedDestrustor called.Deleting···u1(6426,0x1143ee5c0) malloc: *** set a breakpoint in malloc_error_break to debugDestrustor called.Destrustor called. 结果分析：程序中ArrayofPoint2是从ArrayofPoint1复制过来的，二者的初始状态肯定是一样的。但是当程序通过move函数移动ArrayofPoint1的第一组点后，ArrayofPoint2的第二组点也被移动到相同的位置，显示同样的结果。如图1所示有图可以看出，在使用默认构造函数将两个对象的数据简单复制后，不仅具有相同的数值，两个对象还指向同一内存地址，所以在移动ArrayofPoint1时，也影响到ArrayofPoint2。并且程序结束时会将这部分空间释放两次，导致运行错误！解决办法是编写复制构造函数来实现“深复制”。在ArrayofPoints类中添加声明一个复制拷贝函数；12345ArrayofPoints::ArrayofPoints(const ArrayofPoints &amp;v) &#123; size=v.size; points=new Point[size]; for(int i=0;i&lt;size;i++) points[i]=v.points[i]; 修改后运行结果如下：123456789101112131415161718please enter the count of points:2default Construstor called.default Construstor called.default Construstor called.default Construstor called.Copy of pointsArray1:point_0 of array2:5,10point_1 of array2:15,20----------------我是可爱的分隔符--------------after the moving of pointsArray1point_0 of array2:5,10point_1 of array2:15,20Deleting···Destrustor called.Destrustor called.Deleting···Destrustor called.Destrustor called. 此时深复制的效果示意图如下：最后引入看到的一个关于浅复制与深拷贝的笑话来加深大家对这部分的印象 考试时小明抄了小红的卷子顺便把名字也抄上了，发成绩时小红的成绩单上登过两次成绩而小明甚至没有自己的成绩单，这个叫浅拷贝。考试时小明抄完小红的卷子然后写了自己的名字，发成绩时各领各的成绩单，这个叫深拷贝。]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之装饰器Decorator]]></title>
    <url>%2F2019%2F06%2F16%2Fdecorator%2F</url>
    <content type="text"><![CDATA[设计模式作为面对各种问题进行提炼和抽象而形成的解决方案，常见可分为三个大类： 创建类设计模式 结构类设计模式 行为类设计模式而今天主要讲述到是结构类设计模式中的装饰器模式。 一.目的为一个对象动态的附加额外的职责。装饰器（Decorators)提供除了用子类的另外一种灵活的扩充功能方法。就增加功能而言，装饰器模式比生成子类更为灵活；它允许向一个现有的对象添加新的功能，同时又不改变其结构。二. UML类图解析装饰器模式的UML类图如下抽象组件（Component）定义一个对象接口，使得装饰对象能动态加入到组件中。具体组件（ConcreteComponent）定义一个能附加职责的类。抽象装饰器（Decorator）维持组件对象的一个引用，并定义与组件接口一致的接口。具体装饰器（ConcreteDecorator）添加职责给组件。三.实例–点餐系统先定义一类饮料的基类 Beverage ，为不同饮料做好一个接口。每种不同的饮料（Cole，milk）继承这个基类，覆盖各自的特性，比如描述，价格等。12345678910111213141516171819202122class Beverage(): name = &quot;&quot; price = 0.0 type = &quot;BEVERAGE&quot; def getPrice(self): return self.price def setPrice(self, price): self.price = price def getName(self): return self.nameclass coke(Beverage): def __init__(self): self.name = &quot;coke&quot; self.price = 4.0class milk(Beverage): def __init__(self): self.name = &quot;milk&quot; self.price = 5.0 除了基本配置，快餐店卖可乐时，可以选择加冰，如果加冰的话，要在原价上加0.3元；卖牛奶时，可以选择加糖，如果加糖的话，要原价上加0.5元,每种饮料都有很多中附加，，加糖，加冰等等;这里我们定义装饰器：123456789101112131415161718192021class drinkDecorator(): def getName(self): pass def getPrice(self): passclass iceDecorator(drinkDecorator): def __init__(self,beverage): self.beverage=beverage def getName(self): return self.beverage.getName()+&quot; +ice&quot; def getPrice(self): return self.beverage.getPrice()+0.3 class sugarDecorator(drinkDecorator): def __init__(self,beverage): self.beverage=beverage def getName(self): return self.beverage.getName()+&quot; +sugar&quot; def getPrice(self): return self.beverage.getPrice()+0.5 在具体的业务场景中，就可以与饮料类进行关联。以牛奶+糖为例，示例业务场景如下：1234567if __name__==&quot;__main__&quot;: Telunsu=milk() print (&quot;Name:%s&quot;%Telunsu.getName()) print (&quot;Price:%s&quot;%Telunsu.getPrice()) sugar_milk=sugarDecorator(Telunsu) print (&quot;Name:%s&quot; % sugar_milk.getName()) print (&quot;Price:%s&quot; % sugar_milk.getPrice()) 打印结果如下：12345D:\ACAno\python.exe E:/untitled1/11.pyName:milkPrice:5.0Name:milk +sugarPrice:5.5 四.优点与场景优点：1、抽象装饰器和具体被装饰的对象实现同一个接口抽象装饰器里面要持有接口对象，以便请求传递,方便动态的扩展功能，且提供了比继承更多的灵活性；2、Python的装饰器模式是实现Aspect Oriented Programming（AOP）的一种方式应用场景：1、需要动态添加一个类的功能，动态撤销，如本例 五.reference Python与设计模式–装饰器模式 Python与设计模式 zhihu-如何理解Python装饰器？ AOP]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Design Pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epsilon-Greedy 总结]]></title>
    <url>%2F2019%2F06%2F16%2Feplison-greedy%2F</url>
    <content type="text"><![CDATA[新年伊始，万事待兴，很高兴我即将进入学生时代最忙碌的一年（论文+offer的双重压力）；寒假期间有读&lt;刻意练习&gt;这本书，书中最让我受益的信念便是杰出不是一种天赋，而是一种技巧；这种技巧，你我都可以掌握;而这样的一种锻炼自我的方式需要持之以恒的养成，在新的一年我也会坚持博客更新，愿你我都能拥有这样终生学习的能力；话不多数，让我们进入今天讨论的主题。 (一)bandit算法原理 作为在线学习的一种，一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来进行在线优化。其中算法模型参数是根据观察数据不断演变。常用于增强学习，智能决策等问题中。 以传统的k-摇臂赌博机,（ k-armed bandit）为例： 这种赌博机有k个摇臂, 玩家投一个游戏币以后可以按下任意一个摇臂, 每个摇臂以一定的概率吐出硬币, 作为奖赏. 但这个概率玩家并不知道. 玩家的目标是通过一定的策略获得最大化的累积奖赏. 一般我们假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p，然后不断地试验，去估计出一个置信度较高的概率p的概率分布就能近似解决这个问题。如何能估计概率p的概率分布呢？ 这里简单的介绍几种常用的bandit算法 (1)navie完全朴素算法先从最简单的开始，先试几次，每个臂都有了均值之后，一直选均值最大那个臂； (2)Thompson sampling算法每次选择臂的方式是：用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。 code：choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins)) (3)Upper Confidence Bound(置信区间上界)先对每一个臂都试一遍之后，每次选择以下值最大的那个臂 （二）贪心算法思想贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。 ==基本步骤== 建立数学模型来描述问题； 把求解的问题分成若干个子问题； .对每一子问题求解，得到子问题的局部最优解； 把子问题的解局部最优解合成原来解问题的一个解。 在算法中仅依据当前已有的信息就做出选择，并且以后都不会改变这次选择。（这是和动态规划法的主要差别） (三)EE问题这两个”E”，其中一个代表“exploit”,中文可译作“利用”；另一个代表“Explore”,中文可译作“探索” 回到K摇臂机问题，由于并不清楚每个摇臂的吐钱的概率分布。那么，如果你想要最大化收益，你该怎么办呢？ 通常来说，你内心可能有两种好的决策：1、找到某一个收益还不错的摇臂，然后坚持摇这个；2、不断尝试探索新的老虎机。这个探索的过程中，可能发现更好的老虎机，当然也要承担摇差的老虎机带来损失的风险。 显然，第一种对应的就是“exploit”，第二种对应“explore”，而bandit算法就是要解决这种EE问题，实现最大化收益。 。 核心问题：什么时候探索(Exploration)，什么时候利用 (Exploitation)?请看第四节 (四)epsilon-Greedy Algorithmϵ -贪婪算法是如何在“exploit”和“explore”之间实现权衡，以尽可能实现最大化收益的呢？ 首先，从算法的名称我们知道，这是一种贪婪的算法。纯粹贪婪的算法，放到这种多臂老虎机的场景来看就是每次都选择当前那个最好的臂摇，即使从长远来看可能非常不好。 这个ϵ代表执行执行“探索”的概率。比如设置ϵϵ=0.1，那么就表示有10%的概率会进行“探索”操作，而90%会进行“利用”操作，也就是摇当前最好的臂。如果以摇老虎机臂的过程，用次数来算的话，也就是，每10次操作，仅有1次操作去进行探索——尝试其他的臂。 code url:https://github.com/johnmyleswhite/BanditsBook/tree/master/python/algorithms/epsilon_greedy1234567891011121314151617181920212223242526272829303132import randomdef ind_max(x): m = max(x) return x.index(m)class EpsilonGreedy(): def __init__(self, epsilon, counts, values): self.epsilon = epsilon self.counts = counts self.values = values return def initialize(self, n_arms): self.counts = [0 for col in range(n_arms)] self.values = [0.0 for col in range(n_arms)] return def select_arm(self): if random.random() &gt; self.epsilon: return ind_max(self.values) else: return random.randrange(len(self.values)) def update(self, chosen_arm, reward): self.counts[chosen_arm] = self.counts[chosen_arm] + 1 n = self.counts[chosen_arm] value = self.values[chosen_arm] new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward self.values[chosen_arm] = new_value return (五）参考[1]https://zhuanlan.zhihu.com/p/38739197[2]http://www.voidcn.com/article/p-ejmpwufg-bpz.html[3]https://blog.csdn.net/qjf42/article/details/79655483]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从交叉熵（cross entropy loss）看信息论]]></title>
    <url>%2F2019%2F06%2F16%2F%E4%BA%A4%E5%8F%89%E7%86%B5%2F</url>
    <content type="text"><![CDATA[信息论真的是老生常谈的话题，生活在如今网络高速发达的时代，用户每天都会接收到数以万计的信息，就连我的研究方向也与信息传播有关，所以如何能从从不同的视角分享学习信息量背后的知识，是我写这篇博文的初衷。 （一）信息量学过计算机或者通信原理的童鞋都对香农甚是了解，其提出了一个定量衡量信息量的公式： 假设x是一个离散型随机变量，其取值集合为X,概率分布函数p(x)=Pr(X=x),x∈X,则定义事件X=$x_0$的信息量 I($x_0$)为：$I(x_0)=-log(P(x_0))$ 信息量是对事件发生概率的度量，一个事件发生的概率越低，则这个事件包含的信息量越大，这跟我们直观上的认知也是吻合的，越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。这里举一个有趣的例子增进了解 事件A：巴西队进入了2018世界杯决赛圈。事件B：中国队进入了2018世界杯决赛圈。 仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。从这里我们就可以顺理成章的引出熵的概念$\downarrow$ （二）熵熵是一种对不确定性的方法，对于存在不确定性的系统，熵越大表示该系统的不确定性越大，熵为0表示没有任何不确定性。熵H(x)的定义如下： 对于某个事件，有n种可能性发生，每一种可能性都有一个概率$p(x_i)$$H(X)=−\sum_{n=1}^Np(x_i)\ln(p(x_i))$ 序号 事件 概率p 信息量I 1 今天正常上课 0.7 -log(p(A))=0.36 2 今天不上课 0.2 -log(p(B))=1.61 3 学校爆炸了 0.1 -log(p(C))=2.30 则可以计算出H(X)=0.804 （三）相对熵（KL散度）相对熵:简而言之其用来衡量两个取值为正的函数或概率分布之间的差异 如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异 公式即：$KL(f(x) || g(x)) = \sum_{ x \in X} f(x) * \log_2 \frac{f(x)}{g(x)}$假设我们想知道某个策略和最优策略之间的差异，我们就可以用相对熵来衡量这两者之间的差异。即相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略）$KL（p || q） = H（p，q） - H（p） = \sum_{k=1}^N p_k \log_2 \frac{1}{q_k} - \sum_{k=1}^N p_k \log_2 \frac{1}{p_k} = \sum_{k=1}^N p_k \log_2 \frac{p_k}{q_k}$ （四）交叉熵函数 $H(p,q)=−\sum_{n=1}^Np(x_i)log(q(x_i))$ (1)性质a)非负性。（所以我们的目标就是最小化代价函数）b)当真实输出a与期望输出y接近的时候，代价函数接近于0.(比如y=0，loss～0；y=1，loss~1时，代价函数都接近0)。此外它可以克服方差代价函数更新权重过慢的问题 (2)推导这里就不班门弄斧啦，推荐红石大佬的步骤，清晰明了$\longrightarrow$简单的交叉熵损失函数 (2)交叉熵和均方误差的区别交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于： 交叉熵适用于分类问题，结果是离散的类别（如图片分类）， 均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】 在使用sigmod激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含sigmod函数，而不包含sigmod函数的导数 Reference(1)About loss functions(2)神经网络Loss损失函数总结(3)如何通俗的解释交叉熵与相对熵?]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>loss function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的异常处理机制]]></title>
    <url>%2F2019%2F06%2F16%2F%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[之前看莫神关于python中异常处理的讲解后，一直想要更深入的了解其中机制的运转，所以想借着今天复习一波。 在Python当中，若一个程序在运行的时候出错，Python解释器会自动的在出错的地方生成一个异常对象，而后Python解释器会自动的在出错地方的附近寻找有没有对这个异常对象处理的代码，所谓异常处理代码就是try……except语句，如果没有，Python解释器会自动的将这个异常对象抛给其调用函数，就这样层层抛出，如果在main当中也没有对这个异常对象处理的代码，Python解释器（实际上是操作系统）最后会做一个简单粗暴的处理，将整个程序给终止掉，并将错误的信息在显示屏上输出。 （一）try语句的两种风格：1）：处理异常（try/except/else）123456try: print(&apos;xxx&apos;) #如果try的子语句能够实现，则执行这个语句。整个流程走向else，然后控制流通过整个try语句。except: print(&apos;error&apos;)else: print(1) 若正常则输出： 12XXX 1 否则执行except就不会执行else，输出： error 2）：异常与否都将执行最后的代码（try/finally）123456try: print(xx)except: print(&apos;error&apos;)finally: print(&apos;ok&apos;) 若出错则输出：12errorok 否则输出：12XXok （二）处理多个异常与exception异常当处理多个异常时：1234567891011try: codeexcept Error1 as e: #处理Error1异常 print(e)except Error2 as e: #处理Error2异常 print(e) else: print &apos;no error!&apos;finally: print &apos;finally...&apos;print &apos;END&apos; 当使用exception异常时：123456try: codeexcept (Error1,Error2,...) as e: print(e)except Exception as e: #用Exception表示一下子抓住所有异常，这个一般情况下建议在异常最后面用，用在最后抓未知的异常 print(e) 其中值得注意的是：Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如：123456try: foo()except StandardError, e: print &apos;StandardError&apos;except ValueError, e: print &apos;ValueError&apos; 第二个except永远也捕获不到ValueError，因为ValueError是StandardError的子类，如果有，也被第一个except给捕获了。Python所有的错误都是从BaseException类派生的，常见的错误类型和继承关系点击这里 (三)自定义异常通过创建一个新的异常类，程序可以命名它们自己的异常。异常应该是典型的继承自Exception类，通过直接或间接的方式。以下为与RuntimeError相关的实例,实例中创建了一个类，基类为RuntimeError，用于在异常触发时输出更多的信息。 123class Networkerror(RuntimeError): def __init__(self, arg): self.args = arg 在你定义以上类后，你可以触发该异常 1234 try: raise Networkerror(&quot;Bad hostname&quot;)except Networkerror,a: print a.args 在try语句块中，用户自定义的异常后执行except块语句，变量 a是用于创建Networkerror类的实例 (四)Python内置的logging模块记录错误 如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去 123456789101112import loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar(&apos;0&apos;) except Exception as e: logging.exception(e)main()print(&apos;END&apos;) 同样是出错，但程序打印完错误信息后会继续执行，并正常退出123456789101112D:\ACAno\python.exe E:/untitled1/day20.pyENDERROR:root:division by zeroTraceback (most recent call last): File &quot;E:/untitled1/day20.py&quot;, line 11, in main bar(&apos;0&apos;) File &quot;E:/untitled1/day20.py&quot;, line 7, in bar return foo(s) * 2 File &quot;E:/untitled1/day20.py&quot;, line 4, in foo return 10 / int(s)ZeroDivisionError: division by zero通过配置，logging还可以把错误记录到日志文件里，方便事后排查]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯网络及朴素贝叶斯（二）]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[哈哈哈我是挖坑的小当家，上次挖的贝叶斯当然还是要补上，中间有次周末断更是因为网球结业考试！万幸的是考试安全飘过了。本着世间万物皆可贝叶斯的原则（just kidding）话不多说让我们赶紧进入今天的贝叶斯网络和朴素贝叶斯的介绍 (一)贝叶斯网络 贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)，是一种概率图模型，于1985年由Judea Pearl首先提出。 对于贝叶斯网络，我们可以用两种方法来看待它：(1)首先贝叶斯网表达了各个节点间的条件独立关系，我们可以直观的从贝叶斯网当中得出属性间的条件独立以及依赖关系；(2)另外可以认为贝叶斯网用另一种形式表示出了事件的联合概率分布，根据贝叶斯网的网络结构以及条件概率表（CPT）我们可以快速得到每个基本事件（所有属性值的一个组合）的概率。贝叶斯学习理论利用先验知识和样本数据来获得对未知样本的估计，而概率（包括联合概率和条件概率）是先验信息和样本数据信息在贝叶斯学习理论当中的表现形式。 它是一种模拟人类推理过程中因果关系的不确定性处理模型，其网络拓朴结构是一个有向无环图(DAG)。 贝叶斯网络的有向无环图中的节点表示随机变量。它们能够是可观察到的变量，或隐变量、未知參数等。觉得有因果关系（或非条件独立）的变量或命题则用箭头来连接。类似生活中举例如下图若两个节点间以一个单箭头连接在一起，表示当中一个节点是“因(parents)”。另一个是“果(children)”，两节点就会产生一个条件概率值。若有不理解之处可参考这篇博文贝叶斯网络–概率图模型 总而言之，连接两个节点的箭头代表此两个随机变量是具有因果关系，或非条件独立。 （1）贝叶斯网络的3种结构形式贝叶斯网络的第一种结构形式1：head-to-head： 有：P(a,b,c) = P(a)P(b)P(c|a,b)成立，化简后可得 即在c未知的条件下，a、b被阻断(blocked)，是独立的，称之为head-to-head条件独立 2：tail-to-tail 贝叶斯网络的第二种结构形式 考虑c未知，跟c已知这两种情况： 在c未知的时候，有：P(a,b,c)=P(c)P(a|c)P(b|c)，此时，没法得出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。在c已知的时候，有：P(a,b|c)=P(a,b,c)/P(c)，然后将P(a,b,c)=P(c)P(a|c)P(b|c)带入式子中。得到：P(a,b|c)=P(a,b,c)/P(c) = P(c)P(a|c)P(b|c) / P(c) = P(a|c)*P(b|c)。即c已知时，a、b独立。 所以，在c给定的条件下，a，b被阻断(blocked)，是独立的。称之为tail-to-tail条件独立 3：head-to-tail 贝叶斯网络的第三种结构形式例如以下图所看到的： 还是分c未知跟c已知这两种情况： c未知时。有：P(a,b,c)=P(a)P(c|a)P(b|c)。但无法推出P(a,b) = P(a)P(b)，即c未知时。a、b不独立。c已知时，有：P(a,b|c)=P(a,b,c)/P(c)，且依据P(a,c) = P(a)P(c|a) = P(c)P(a|c)，可化简得到： 所以，在c给定的条件下，a，b被阻断(blocked)。是独立的，称之为head-to-tail条件独立。总结这个head-to-tail其实就是一个链式网络 （2）贝叶斯网络的实例这方面我看到很多大神在知乎上已经有许多有趣的例子，大家可以移步去贝叶斯网络及其应用，这里我就不多加赘述。 (二)朴素贝叶斯关于朴素贝叶斯我看到过很多解释 但是其中一位大佬的解释最让人简洁明了（超级好玩） 朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。 所以朴素贝叶斯中的朴素一词的来源就是假设各特征之间相互独立。这一假设使得朴素贝叶斯算法简洁高效，但有时会牺牲一定的分类准确率。贝叶斯公式我们已经在上次博文中介绍过，其转化为分类-特征公式其实就是整个朴素贝叶斯分类分为三个阶段如下图： 1准备工作阶段，任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 2分类器训练阶段: 这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 3应用阶段:这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 其中李航博士的《统计学习方法》对此有细致的研究，特意download了大神的coding本地运行了一遍数据集采用的minist数据集，代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120import copyimport pandas as pdimport numpy as npimport cv2import mathimport randomimport timefrom sklearn.model_selection import KFoldfrom sklearn.model_selection import train_test_split# 二值化def binaryzation(img): cv_img = img.astype(np.uint8) cv2.threshold(cv_img, 50, 1, cv2.THRESH_BINARY_INV, cv_img) return cv_imgdef Train(trainset, train_labels): prior_probability = np.zeros(class_num) # 先验概率 conditional_probability = np.zeros((class_num, feature_len, 2)) # 条件概率 # 计算先验概率及条件概率 for i in range(len(train_labels)): img = binaryzation(trainset[i]) # 图片二值化 label = train_labels[i] (prior_probability)[label]=1+prior_probability[label] for j in range(feature_len): conditional_probability[label][j][img[j]] += 1 # 将概率归到[1.10001] for i in range(class_num): for j in range(feature_len): # 经过二值化后图像只有0，1两种取值 pix_0 = conditional_probability[i][j][0] pix_1 = conditional_probability[i][j][1] # 计算0，1像素点对应的条件概率 probalility_0 = (float(pix_0) / float(pix_0 + pix_1)) * 1000000 + 1 probalility_1 = (float(pix_1) / float(pix_0 + pix_1)) * 1000000 + 1 conditional_probability[i][j][0] = probalility_0 conditional_probability[i][j][1] = probalility_1 return prior_probability, conditional_probability# 计算概率def calculate_probability(img, label): probability = int(prior_probability[label]) for i in range(len(img)): probability *= int(conditional_probability[label][i][img[i]]) return probabilitydef Predict(testset, prior_probability, conditional_probability): predict = [] for img in testset: # 图像二值化 img = binaryzation(img) max_label = 0 max_probability = calculate_probability(img, 0) for j in range(1, 10): probability = calculate_probability(img, j) if max_probability &lt; probability: max_label = j max_probability = probability predict.append(max_label) return np.array(predict)class_num = 10feature_len = 784if __name__ == &apos;__main__&apos;: print(&apos;Start read data&apos;) time_1 = time.time() delimiter = &quot;\t&quot; # 这样读入： # df=pd.read_csv(&apos;path&apos;,delimiter=&quot;\t&quot;) raw_data = pd.read_csv(&apos;D:/ACAno/data/train.csv&apos;, delimiter=&quot;\t&quot;) data = raw_data.values imgs = data[0::, 1::] labels = data[::, 0] # 选取 2/3 数据作为训练集， 1/3 数据作为测试集 train_features, test_features, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.33, random_state=23323) # print train_features.shape # print train_features.shape time_2 = time.time() print(&apos;read data cost &apos;, time_2 - time_1, &apos; second&apos;, &apos;\n&apos;) print(&apos;Start training&apos;) prior_probability, conditional_probability = Train(train_features, train_labels) time_3 = time.time() print(&apos;training cost &apos;, time_3 - time_2, &apos; second&apos;, &apos;\n&apos;) print(&apos;Start predicting&apos;) test_predict = Predict(test_features, prior_probability, conditional_probability) time_4 = time.time() print(&apos;predicting cost &apos;, time_4 - time_3, &apos; second&apos;, &apos;\n&apos;) score = accuracy_score(test_labels, test_predict) print(&quot;The accruacy socre is &quot;, score) 总结其实自己也是最近常用到求解贝叶斯纳什均衡才开始认真的的学习这方面的知识，参考了很多博文后加深了点自己对贝叶斯公式及其原理的了解。希望大家都能够喜欢自己的研究，2018即将过去，我会在最近补上我近期的生活随笔及感悟，并且预告下周我会介绍bandit算法中的epsilon-greedy，欢迎大家相互交流（主要是我学您哈哈哈哈）]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Bayesian network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[混合策略下纳什均衡的求解方法]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5nash%E6%B1%82%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[因为自己的研究方向是博弈论问题，之前在做混合策略+演化博弈（现在不做了）求解的时候，参考了很多关于纳什均衡和策略求解的方法，今天在这里会简单的叙述：一般情况下 ，我们可以如何去快速求解混合策略下的NE（Nash Equilibrium） 问题很简单，那么我来给大家精简介绍下相关概念吧。 一 概念介绍 1）纳什均衡 纳什均衡是指博弈中这样的局面，对于每个参与者来说，只要其他人不改变策略，他就无法改善自己的状况 谈到博弈，都不得不提一位人物纳什,也正因为他，才有我今天能研究的课题（==鞠躬==） 约翰·纳什，生于1928年6月13日。著名经济学家、博弈论创始人、《美丽心灵》男主角原型。前麻省理工学院助教，后任普林斯顿大学数学系教授，主要研究博弈论、微分几何学和偏微分方程。由于他与另外两位数学家（经济学家，约翰·C·海萨尼和莱因哈德·泽尔腾）在非合作博弈的均衡分析理论方面做出了开创性的贡献，对博弈论和经济学产生了重大影响，而获得1994年诺贝尔经济学奖。2）纯策略与混合策略纯策略混合策略混合策略纳什均衡是面对其他博弈者选择的不确定性的一个理性对策，其主要特征是作为混合策略一部分的每一个纯策略有相同的期望值，否则，一个博弈者会选择那个期望值最高的策略而排除所有其他策略，这意味着原初的状态不是一个均衡。 接下来让我们用公式来简化 Ⅰ与Ⅱ是一个博弈的两个局中人。他们的纯策略集(1.2.1)分别记为： S={s1,s2,…sn}和T={t1,t2, …tm} （1.2.1） x与y是两个概率向量，即: x=(x1,x2, …xn)T；xi≥0（i=1,2, …n）;∑xi = 1 y=(y1,y2, …ym)T；yj≥0（j=1,2, …m）;∑yj = 1若 x表示对局中人Ⅰ的纯策略集S的全体策略的一种概率选择；y表示对局中人Ⅱ的纯策略集T的全体策略的一种概率配置，即： Ⅰ 以概率x1选择策略s1，以概率x2选择策略s2，……以概率xn选择策略sn。 Ⅱ 以概率y1选择策略t1，以概率y2选择策略t2，……以概率ym选择策略tm。 则 称x为局中人Ⅰ的混合策略；称y为局中人Ⅱ的混合策略。 因此 其意义是表示局中人对各个纯策略的偏好程度，或是对多次博弈达到均衡结局的各个纯策略选择的概率估计。 二例题解析在搜集资料的时候看到很多人都推荐 《如何制定一个必赢的赌博规则》这篇文章，觉得新奇有趣，还有大佬复现代码，话不多说，来举一波硬币正反的问题。 假如你正在图书馆枯坐，一位陌生美女主动过来和你搭讪，并要求和你一起玩个数学游戏。美女提议：“让我们各自亮出硬币的一面，或正或反。如果我们都是正面，那么我给你3元，如果我们都是反面，我给你1元，剩下的情况你给我2元就可以了。”那么该不该和这位姑娘玩这个游戏呢？这基本是废话，当然该。问题是，这个游戏公平吗？ 情景一出，我们便会在心里嘀咕一声：好好地美女找上我，还要和我做游戏！怎么这情况隔着八里地都能闻着忽悠的味道。但是我们转念一算按我们平时正常想是： 两面都一样（或正或反）概率为 1/4+1/4，则其数学期望1/4 3 + 1/4 1 = 1 ，而一正一反的数学期望也是1/2 * 2 = 1 貌似是公平的，实际则不然。问题就出在硬币是我们人为控制的，想正面就正面，想反面就反面，而上述情况只应在抛硬币的时候才成立依据我们上文的介绍，玩家可以使用纯策略(比如一直出正面或者一直出反面)，使得每人都赚得最多或亏得最少；或者是混合策略纳什均衡，而在这个游戏中，便应该采用混合策略纳什均衡。 假设我们出正面的概率是x，反面的概率是1-x，美女出正面的概率是y，反面的概率是1-y。为了使利益最大化，应该在对手出正面或反面的时候我们的收益都相等 便有如下混合策略矩阵：对玩家自己而言：3x + (-2)*(1-x)=(-2) * x + 1*( 1-x )解得x=3/8对美女而言：-3y + 2( 1-y)= 2y+ (-1) * ( 1-y)解得y=3/8即美女每次的期望收益是：2(1-y)- 3y = 1/8元摘录主要代码验证如下：1234567891011121314151617181920212223242526272829303132333435#define SUM_COUNT 1000int my[SUM_COUNT],your[SUM_COUNT];int my_money_sum = 0,your_money_sum = 0;/*数组my美女 ，your 模拟每次出手时两人的硬币正反面，正面为1 ，反面为 0 */void get_rand(int sum , int One_count)&#123; int i = 0; int count = 0; for (i =0 ;i&lt;sum; i++) &#123; your[i] = rand()%2; my[i] = rand() % 2; if(my[i] ==1 ) &#123;count++;&#125; if(count &gt; One_count) &#123;my[i] = 0;&#125; &#125;&#125;void count_the_money(int sum)/*my_money_sum 和 your_money_sum 为双方最后的所得*/&#123; int i; for(i = 0; i&lt;sum; i++) &#123; if( my[i] == your[i]) &#123; if (my[i] == 1) &#123;your_money_sum += 3;&#125; if(my[i] == 0) &#123;your_money_sum +=1;&#125; &#125; else if( my[i] != your[i]) &#123;my_money_sum += 2;&#125; &#125; &#125; 结果截图小结：在双方都采取最优策略的情况下，平均每次美女赢1/8元。而我们则亏1/8元。（其中美女保持出正面的频数在3/8） 三 总结当然本文只是介绍了混合策略纳什均衡的一种方法，还可以从盈利函数 线性规划等方法来尝试求解。而我本人也是在研究博弈论中随机博弈的相关问题，有很多不足之处，希望大家多帮助，thanks.]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Nash Equilibrium</tag>
        <tag>Game theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年尾]]></title>
    <url>%2F2019%2F06%2F16%2F%E7%B5%AE%E7%B5%AE%E5%8F%A8%E5%8F%A81%2F</url>
    <content type="text"><![CDATA[叮叮当叮叮当，铃儿响叮当！临近2018年尾声，我的第一篇生活小随文终于在千呼万唤中始出来。感觉学业和生活上的事情接踵而至，自己已经忙得不亦乐乎。当然细心地你已经发现我对busy这件事情的量化结果是乐，我一直秉持着这样的信念 ： 人年轻时期 忙即营养 当然这里的“年轻”因人而异，有的人而立之年便暮霭程程，有的人半百仍倍感青春健在；同理，不同的人对于忙的定义也千差万别，这里自然是不必深究。 而令我真正坐下来去总结这段时期的点滴生活的契机，是前不久知乎上一个热门话题：2018年，你有哪些特想撤回却无法撤回的操作？？ 我私以为这样的发人深省的问题一般匿名者居多，且多是些与世界分享你刚编的水文，意料之外的是我竟然不落俗套的喜欢这些故事里的桥段，真实又有趣。 所以当我静下心去思考自己的2018：出现了哪些惊喜与懊恼，到底有多少捧腹大笑的场景以及多少不堪回首的剧目，对于愉悦的片段是否想要保留，那不开心的事件又是否需要一键delete？很多并不真实的想法会飞速的在我的脑海里一瞬即逝，然后时间把这一切都凝固成一个种子，缓缓地细心地埋在生命树上那片刻着2018的枝丫里面。 我感激自己所拥有的，也并不懊悔所失去的，因为这都是实实在在的生活，一个真真切切的属于我一个人的2018. 所以没什么想要在这一年里想要撤回的操作。 读到这里 你会不会以为我是个佛系的青年人，不以物喜不以己悲，哈哈哈哈如果你的脑海里真的出现这样的话语，那一定不是我！ 过去即经历，可未来更可期 我对未来可真的大有野心好吗，你要是想问我2019年有什么小目标，我会毫不犹豫斩钉截铁的告诉你： 2019 ，我想要成为有个有温度的人，万事胜意! 祝愿大家 ：扬帆破浪，未来可期]]></content>
      <categories>
        <category>Life perception</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈激活函数]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[今天在写这篇文章前首先恭喜深度学习三巨头成为新晋图灵奖得主！🎉 科普：人工神经网络作为一种帮助计算机识别模式和模拟人类智能的工具在20世纪80年代被引入，但直到21世纪初，只有杨乐昆、辛顿和本吉奥等一小群人仍然坚持使用这种方法。尽管他们的努力也曾遭到怀疑，但他们的想法最终点燃了人工智能社区对神经网络的兴趣，带来了一些最新的重大技术进步。他们的方法现在是该领域的主导范式（dominant paradigm） 我一直在想如果有一天会在博客中写点神经网络方面的tips，应该会从哪里入手最好，直到同学提出神经网络激活函数的疑问，才忽觉可以从激活函数的理解作为切入点浅谈我对人工神经网络这项伟大发明的由衷敬意（同时我也要在这里感谢和汇总到许多大拿的经典观点，在这里向大家致敬） （一）简单神经网络介绍科学总是回归到人的本质去探究，当科学家发现人之所以会思考是因为我们的大脑中具有神经元，这样天然的神经网络使得人体可以根据神经中枢的指令，对外部刺激做出反应。这样的神经网络组成包括有：输入节点，连接，输入与权重的结合，激活函数，输出等为了简化模型，我们约定每种输入只有两种可能：1 或 0。如果所有输入都是1，表示各种条件都成立，输出就是1；如果所有输入都是0，表示条件都不成立，输出就是0。对于人工神经网络的定义诠释，我个人很喜欢知乎精选问题首赞的解答（跳转门-&gt;）：如何简单形象又有趣地讲解神经网络是什么？感兴趣的小伙伴不如细心阅读一番。 （二）激活函数的由来当我们了解到神经网络大概的基本要素时，好奇的你不由会问到为什么我们需要激活函数（activation function）运用其中？这里我们会提到多层神经网络，这也是是我们常用的👇。 多层感知器又叫前馈神经网络，类似神经元以层级结构组织在一起。层数一般是二三层，但是理论上层数是无限的。网络的层就像生物神经元：一层的输出，是下一层的输入。、网络层分为输入层、隐藏层和输出层。多层感知器通常是全连接(fully-connected)的，一层之中的每一个感知器都与下一层的每一个感知器相连接，尽管这不是强制性，但通常是标配。感知器只能表征线性可分的问题，而多层感知器结合非线性的激活函数就突破了这一限制，可以表征更加复杂的决策边界。 而使用激活函数的好处一般有：1）改变之前数据的线性关系：若网络中全部是线性变换，则多层网络可以通过矩阵变换，直接转换成一层神经网络，所以激活函数的存在，使得神经网络的“多层”有了实际的意义！使网络更加强大，增加网络的能力，使它可以学习复杂的事物，复杂的数据，以及表示输入输出之间非线性的复杂的任意函数映射。2）防止数据过大溢出风险：是执行数据的归一化，将输入数据映射到某个范围内，再往下传递，这样做的好处是可以限制数据的扩张。 （三）激活函数的分类类似武林派系之分，传统激活函数大致分为==identity==、==sigmoid系==、==ReLU 及其变体== 1）通过激活函数 Identity，节点的输入等于输出。它完美适合于潜在行为是线性（与线性回归相似）的任务。当存在非线性，单独使用该激活函数是不够的，但它依然可以在最终输出节点上作为激活函数用于回归任务。 2）传统神经网络中最常用的两个激活函数：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid） 从数学上来看,非线性的Sigmoid函数对中央区的信号增益较大,对两侧区的信号增益小,在信号的特征空间映射上,有很好的效果,通过对加权的输入进行非线性组合产生非线性决策边界.从神经科学上来看,中央区酷似神经元的兴奋态,两侧区酷似神经元的抑制态,因而在神经网络学习方面,可以将重点特征推向中央区,将非重点特征推向两侧区. 3）修正线性单元（Rectified linear unit，ReLU）是神经网络中最常用的激活函数。它保留了 step 函数的生物学启发（只有输入超出阈值时神经元才激活）。不过当输入为正的时候，导数不为零，从而允许基于梯度的学习（尽管在 x=0 的时候，导数是未定义的）。使用这个函数能使计算变得很快，因为无论是函数还是其导数都不包含复杂的数学运算；缺点在于在训练的时候,网络很脆弱,很容易出现很多神经元值为0,从而再也训练不动.一般将学习率设置为较小值来避免这种情况的发生.[汇总] （四）小结 激活函数的意义是“让神经网络具备强大的拟合能力”。同时也是AI神经网络给予这些函数新的生命！十几二十年前都不好意思说自己是做AI的，因为觉得容易被当成骗子。三位大拿依然保持学者的本色，恭喜三位，坚守几十年，实至名归！ 同时国内码农界也出了件大事：github飞速star的996,icu项目同样也刺激着大家的眼球。 这里作为IT界一名学生的我，其实也是从社会媒体以及师兄师姐的焦虑中感受良多，这里我想说：计算机科学是一门新兴工业科学，一个研究成果的重要性，是看应用和对改变世界的影响有多少，新的事物带来新的挑战与危机， 希望越来越强大的神经网络可以为科研助力高飞，也希望自己论文能够早中，最后祝愿大家学习快乐。]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅《风中有朵雨做的云》感悟]]></title>
    <url>%2F2019%2F06%2F16%2F%E9%A3%8E%E4%B8%8E%E4%BA%91%2F</url>
    <content type="text"><![CDATA[其实于我而言，风雨云这部电影 是和《我不是药神》一样重量级的影片； 感谢娄烨导演，镜头从拆迁闹事开始，展出一幅幅真实·震撼的生活百景图； 让坐在实验室里，每天三点一线的我，能更切身的体会到‘’外面世界‘’的不易； 看完电影之后百感滋味在心头，当我们遇到人生起伏是是非非，很多决策与用意已经不能仅仅以善恶来评判； 不了解林慧，她欺骗感情婚内出轨，是个十恶不赦的渣女，误杀情敌阿云，包庇女儿自愿入狱，甚至愿意为不相干的家栋以命博姜紫成； 不了解唐奕杰，他贪赃枉法一手遮天，置百姓利益于不顾，是个彻头彻尾的贪官污吏，但是年少时期单纯青涩，向往爱情却求而不得，照顾小诺视如己出； 不了解姜紫成，他表面浪荡不羁，声色犬马，在杀人不眨眼的商界步步为营，为达目的不择手段，是个纯24K的奸商，欺骗阿云却又钟情林慧，或许他感情一直从一，除林慧外皆可抛，却又爱惜小诺，似乎尚存一丝人性； 不了解家栋，影片上演的是忍辱负重，未查出伤父真相小心谨慎，却又极易受人牵绊，自损八百。明明对小诺有情但更也仅此为止，尊乎警队法律，却又沉冤得雪后辞职，迷雾重重； 不了解小诺，外表是游手好闲的富家女孩，一面爱护亲人，对姜叔和母亲的感情心知肚明，却对养护多年的法律意义“父亲”监护人唐主任凶残杀害，不明亲情又向往得到家栋爱情，似乎想要走出浑水却早已越陷越深； 或许正如所言：社会是风，欲望是雨，我们是云 最后推荐影片中饰演唐奕杰演员的回答-&gt;如何评价电影《风中有朵雨做的云》？简直是本年度最佳番外！]]></content>
      <categories>
        <category>Life perception</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从贝叶斯公式到贝叶斯网络（一）]]></title>
    <url>%2F2019%2F06%2F16%2F%E8%B4%9D%E5%8F%B6%E6%96%AF1%2F</url>
    <content type="text"><![CDATA[最近真的是离不开贝叶斯这个科学牛人，他的理论至今仍对科学研究有着极大的影响，所以自己在兴趣与学业的双重感知(鸭梨)下毅然而然的决定要加深自己对贝叶斯理论的了解，奈何知识点太多而自己目前的了解有限，所以本章主要讲述基本的公式，算法思想和场景更多具体而生动的内容咱们下文再续。话不多说让我们一睹为快！ BTW,若观点错误或者引用侵权的欢迎指正交流。 引言 自古以来，人们对一件事情发生或不发生的概率，只有固定的0和1，即要么发生，要么不发生，而不会去考虑某件事情发生的概率有多大，不发生的概率又是多大。比如如果问那时的人们一个问题：“有一个袋子，里面装着若干个白球和黑球，请问从袋子中取得白球的概率是多少？”他们会不假思索告诉你，取出白球和黑球都是一半对一半，非黑即白； 即θ只能有一个值，而且不论你取了多少次，取得白球的概率θ始终都是1/2，即不随观察结果X 的变化而变化。 这种频率派的观点长期统治着人们的观念，直到后来一个名叫Thomas Bayes的人物出现。（*_^当然我国古代著名哲学家庄子老先生的某些思想其实也暗含着贝叶斯理论） (1)贝叶斯方法的提出 托马斯·贝叶斯Thomas Bayes（1702-1763） 生活在18世纪的贝叶斯生前是位受人尊敬英格兰长老会牧师。为了证明上帝的存在，他发明了概率统计学原理，遗憾的是，并不为当时的人们所熟知他的这一美好愿望至死也未能实现。贝叶斯在数学方面主要研究概率论。他首先将归纳推理法 用于概率论基础理论，并创立了贝叶斯统计理论，对于统计决策函数、统计推断、统计的估算等做出了贡献。 在 继续深入解说贝叶斯方法之前，先简单总结下频率派与贝叶斯派各自不同的思考方式： 频率派把须要判断的參数θ看做是固定的未知常数。即概率尽管是未知的，但最起码是确定的一个值，同一时候，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布。贝叶斯派的观点则截然相反。他们觉得參数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是參数的分布。 至此，贝叶斯及贝叶斯派提出了一个思考问题的固定模式： 先验分布 $\Pi(\Theta)$+ 样本信息$X$=&gt; 后验分布$\Pi(\Theta|X)$ （2）概率公式2.1）条件概率 设A,B是两个事件，且P(B)&gt;0,则在事件B发生的条件下，事件A发生的条件概率（conditional probability)为： P(A|B)=P(AB)/P(B) 例如： ① 扔骰子，扔出的点数介于[1,3]称为事件A，扔出的点数介于[2,5]称为事件B，问：B已经发生的条件下，A发生的概率是多少？也即，做一次实验时，即有可能仅发生A，也有可能仅发生B，也有可能AB同时发生； （2.2）联合概率联合概率表示两个事件共同发生的概率。A与B的联合概率表示$P(A∩B)$或者$P(A,B)$。 (2.3) 边缘概率边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。 （2.4）贝叶斯公式紧接着考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。 首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示；其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示；同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。 场景以我们常用的搜索引擎当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“happu”时，系统会猜测你的意图：是不是要搜索“happy”，如下图所示：这便是拼写检查。吴军博士的《数学之美》，Google的拼写检查基于贝叶斯方法。下面以此为例贝叶斯方法，实现”拼写检查”的功能。 用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么”拼写检查”要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求的$P(c|w)$最大值。 根据贝叶斯定理，有：$$p(C|W)=\frac{(P(W|C))(P(C))}{P(W)}$$ 由于对所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化 $(P(W|C))(P(C))$即可。其中： $P(c)$表示某个正确的词的出现”概率”，它可以用”频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离” 所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。 ==介绍尚未结束 欲知详情请听下回分说==]]></content>
      <categories>
        <category>Technical learning</category>
      </categories>
      <tags>
        <tag>Technical log</tag>
        <tag>Bayesian theory</tag>
      </tags>
  </entry>
</search>
